---
title: "Self-organising map (SOM) analysis"
author: "Robert Schlegel"
date: "2020-08-25"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
csl: FMars.csl
bibliography: MHWflux.bib
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.align = 'center',
                      echo = TRUE, warning = FALSE, message = FALSE, 
                      eval = TRUE, tidy = FALSE)
```

## Introduction

This vignette contains the code used to perform the self-organising map (SOM) analysis on the mean synoptic states created in the [data preparation](https://robwschlegel.github.io/MHWflux/data-prep.html) vignette. We'll start by creating custom packets that meet certain experimental criteria before feeding them into a SOM.

```{r libraries}
# Load functions and objects to be used below
source("code/functions.R")
```

## Data packet

In this step we will create a data packet that can be fed directly into the SOM algorithm. This means that it must be converted into a super-wide matrix format. In the first run of this analysis on the NAPA model data it was found that the inclusion of the Labrador Sea complicated the results quite a bit. It was also unclear whether or not the Gulf of St Lawrence (GSL) region should be included in the analysis. So in the second run of this analysis multiple different SOM variations were employed and it was decided that the GSL region should be included.

### Prep synoptic state packets

Up first we must create the synoptic state packets.

```{r unnest-packets, eval=FALSE}
# Set number of cores
  # NB: 50 cores uses too much RAM
registerDoParallel(cores = 20)

# Load needed data
ALL_anom <- readRDS("data/ALL_anom.Rda")
ALL_other <- readRDS("data/ALL_other.Rda")

# Create one big anomaly packet from OISST data
system.time(synoptic_states <- plyr::ddply(OISST_MHW_event, c("region", "event_no"),
                                           data_packet_func, .parallel = T, df = ALL_anom)) # 129 seconds
# Save
saveRDS(synoptic_states, "data/synoptic_states.Rda")

# Create other synoptic states per MHW per variable
doParallel::registerDoParallel(cores = 10) # NB: Be careful here...
system.time(synoptic_states_other <- plyr::ddply(OISST_MHW_event, c("region", "event_no"),
                                                 data_packet_func, .parallel = T, df = ALL_other)) # 212 seconds
# Save
saveRDS(synoptic_states_other, "data/synoptic_states_other.Rda")
```

### Create SOM packet

With all of our data ready we may now prepare and save them for the SOM.

```{r create-SOM-packet, eval=FALSE}
## Create wide data packet that is fed to SOM
system.time(packet <- synoptic_states %>%
              select(region, event_no, synoptic) %>%
              unnest(cols = "synoptic") %>%
              wide_packet_func()) # 79 seconds

# Save
saveRDS(packet, "data/packet.Rda")
```

## Run SOM models

Now we feed the SOM with a function that ingests the data packet and produces results for us. The function below has been greatly expanded on from the previous version of this project and now performs all of the SOM related work in one go. This allowed me to remove a couple hundreds lines of code and text from this vignette.

```{r som-run, eval=FALSE}
# # OISST SOM analysis
packet <- readRDS("data/packet.Rda")
synoptic_states_other <- readRDS("data/synoptic_states_other.Rda")
system.time(som <- som_model_PCI(packet, synoptic_states_other)) # 176 seconds
saveRDS(som, file = "data/som.Rda")
# saveRDS(som, file = "shiny/som.Rda")
```

## Investigate clustering of MHWs

A reviewer of the manuscript noted that the MHWs appear to be clustered closely together both within and across regions. This is by design in the methodology, but just how exactly these events cluster together in the SOM nodes warrants further investigation. The answer of how closely events cluster together within each region was answered in the [MHWs vs. heat flux](https://robwschlegel.github.io/MHWflux/mhw-flux.html#Small_events_and_the_shoaling_MLD) vignette. In this section we will first create an index of MHWs that can be said to be occurring across multiple regions at once. We then look to see how often these events are clustered into the same, or different nodes.

```{r event-cluster}
# Convenience wrapper for creating index of overlapping events
overlap_check <- function(df){
  event_overlap <- OISST_MHW_event %>% 
    dplyr::rename(region_match = region, 
                  event_no_match = event_no,
                  duration_match = duration) %>% 
    filter(date_start <= df$date_peak[1],
           date_end >= df$date_peak[1]) %>%
        mutate(region = df$region,
               event_no = df$event_no,
               duration = df$duration,
               date_peak_orig = df$date_peak) %>% 
    dplyr::select(region, event_no, duration, region_match, event_no_match, duration_match:date_end, date_peak_orig) %>% 
    # Remove self-matched events
    filter(paste(region, event_no) != paste(region_match, event_no_match))
}
event_overlap_res <- plyr::ddply(OISST_MHW_event, c("region", "event_no"), overlap_check, .parallel = T) %>% 
  # Remove duplicates
  mutate(strng1 = paste(region, event_no),
         strng2 = paste(region_match, event_no_match),
         strng3 = paste(region, event_no, region_match, event_no_match),
         strng4 = paste(region_match, event_no_match, region, event_no)) %>%
  mutate(dup_idx = case_when(strng4 %in% strng3 & strng2 == strng1  ~ "check"))
  # filter(!strng2 %in% strng1)
  # filter(distinct(data.frame(.$strng1, .$strng2)))
event_overlap_res$strng1 %in% event_overlap_res$strng2
event_overlap_res[!base::duplicated(event_overlap_res[,c('strng1','strng2')]),]
      # filter(paste(region, event_no) != paste(region_match, event_no_match),
             # paste(region, event_no, region_match, event_no_match) != paste(region_match, event_no_match, region, event_no))

# The events per node
SOM <- readRDS("data/som.Rda")
SOM_info <- SOM$info %>% 
  mutate(node = LETTERS[node])

# How well do these overlaps match up?
event_overlap_node <- event_overlap_res %>% 
  left_join(SOM_info[,c("region", "event_no", "node")], 
            by = c("region", "event_no")) %>% 
  left_join(SOM_info[,c("region", "event_no", "node")], 
            by = c("region_match" = "region", "event_no_match" = "event_no"))

# Get the total count of overlapping events clustered into the same node
paste0(sum(event_overlap_node$node.x == event_overlap_node$node.y), "/",
       nrow(event_overlap_node))
```

This result shows us that of the 27 overlapping event pairs, 19 of them are clustered into the same nodes as one another. Looking through the results manually one may see that of the eight events that are not clustered together, one of the events is very short (<= 8 days), and with one exception (11 days) the other event is much longer (>= 17 days). We can conclude from this that when synoptic scale patterns are causing multiple MHWs simultaneously across regions that the SOM clusters these correctly, with the exception of when one of the events is very short. This is likely because the cause of the much shorter event is a more transient driver that isn't resolved as clearly in the mean synoptic state for the longer event.

Another minor point raised by a reviewer was that the atmospheric pattern in Node I appears to precede that of Node L, and they wanted to know if the MHWs clustered into Node I actually were coming before those events in Node L, and that these two nodes were really just a continuation of a single atmospheric pattern. To determine this we find the gaps in occurrence of the start, peak, and end dates of the MHWs between these nodes. First, we can simply look at the results from the above chunk and confirm that there are indeed no events in Node I that overlap in time with Node L. In fact, Node L is the only node with no events that overlap between regions. Meaning that the pattern in Node L has an extremely localised effect on whichever region it is triggering a MHW in.

```{r node-I-L}
# Prep data
OISST_MHW_event_node <- left_join(OISST_MHW_event, 
                                  SOM_info[,c("region", "event_no", "season_peak", "node")], 
                                  by = c("region", "event_no", "season" = "season_peak")) %>% 
  dplyr::select(region, event_no, node, duration, date_start:date_end)
events_I <- filter(OISST_MHW_event_node, node == "I")
events_L <- filter(OISST_MHW_event_node, node == "L")

# Find gaps in time between events
```


## References

