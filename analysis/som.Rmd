---
title: "Self-organising map (SOM) analysis"
author: "Robert Schlegel"
date: "2020-08-25"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
csl: FMars.csl
bibliography: MHWflux.bib
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.align = 'center',
                      echo = TRUE, warning = FALSE, message = FALSE, 
                      eval = TRUE, tidy = FALSE)
```

## Introduction

This vignette contains the code used to perform the self-organising map (SOM) analysis on the mean synoptic states created in the [data preparation](https://robwschlegel.github.io/MHWflux/data-prep.html) vignette. We'll start by creating custom packets that meet certain experimental criteria before feeding them into a SOM. The full summary of the results may be seen in the [Node summary vignette](https://robwschlegel.github.io/MHWflux/node-summary.html).

```{r libraries}
# Load functions and objects to be used below
source("code/functions.R")
```

## Data packet

In this step we will create a data packet that can be fed directly into the SOM algorithm. This means that it must be converted into a super-wide matrix format. In the first run of this analysis on the NAPA model data it was found that the inclusion of the Labrador Sea complicated the results quite a bit. It was also unclear whether or not the Gulf of St Lawrence (gsl) region should be included in the analysis. So in the second run of this analysis multiple different SOM variations were employed and it was decided that the gsl region should be included.

### Prep synoptic state packets

Up first we must create the synoptic state packets.

```{r unnest-packets, eval=FALSE}
# Set number of cores
  # NB: 50 cores uses too much RAM
registerDoParallel(cores = 20)

# Load needed data
ALL_anom <- readRDS("data/ALL_anom.Rda")
ALL_other <- readRDS("data/ALL_other.Rda")

# Create one big anomaly packet from OISST data
system.time(synoptic_states <- plyr::ddply(OISST_MHW_event, c("region", "event_no"),
                                           data_packet_func, .parallel = T, df = ALL_anom)) # 129 seconds
# Save
saveRDS(synoptic_states, "data/synoptic_states.Rda")

# Create other synoptic states per MHW per variable
doParallel::registerDoParallel(cores = 10) # NB: Be careful here...
system.time(synoptic_states_other <- plyr::ddply(OISST_MHW_event, c("region", "event_no"),
                                                 data_packet_func, .parallel = T, df = ALL_other)) # 212 seconds
# Save
saveRDS(synoptic_states_other, "data/synoptic_states_other.Rda")
```

### Create SOM packet

With all of our data ready we may now prepare and save them for the SOM.

```{r create-SOM-packet, eval=FALSE}
## Create wide data packet that is fed to SOM
system.time(packet <- synoptic_states %>%
              select(region, event_no, synoptic) %>%
              unnest(cols = "synoptic") %>%
              wide_packet_func()) # 79 seconds
# Save
saveRDS(packet, "data/packet.Rda")
```

## Run SOM models

Now we feed the SOM with a function that ingests the data packet and produces results for us. The function below has been greatly expanded on from the previous version of this project and now performs all of the SOM related work in one go. This allowed me to remove a couple hundreds lines of code and text from this vignette.

```{r som-run, eval=FALSE}
# # OISST SOM analysis
packet <- readRDS("data/packet.Rda")
synoptic_states_other <- readRDS("data/synoptic_states_other.Rda")
system.time(som <- som_model_PCI(packet, synoptic_states_other)) # 176 seconds
saveRDS(som, file = "data/som.Rda")
saveRDS(som, file = "shiny/som.Rda")
```

## Correlations

The code used for the MHWNWA project was also used on the GLORYS MHW results to create a SOM for the GLORYS data. I had a look at them and they were fine, but after further thinking we have decided to use the OISST. These SOM nodes are used below to cluster the correlation results to see how the differ based on the SOM. For reference to the results below let's see what the SOM results for the regions + seasons and Air temperature + MSLP look like.

```{r SOM}
# Load the SOM from the MHWNWA
SOM <- readRDS("data/som.Rda")

# Grab only the node info
SOM_info <- SOM$info

# Load correlations
ALL_cor_wide <- readRDS("data/ALL_cor.Rda") %>% 
  ungroup() %>% 
  filter(Parameter1 == "sst") %>% 
  dplyr::select(region:ts, Parameter2, r, n_Obs) %>% 
  pivot_wider(values_from = r, names_from = Parameter2)

# Combine MHW metrics and correlation results
events_cor_prep <- OISST_MHW_event %>% 
  dplyr::select(region, season, event_no, duration, intensity_mean, intensity_max, 
                intensity_cumulative, rate_onset, rate_decline) %>% 
  left_join(ALL_cor_wide, by = c("region", "season", "event_no")) %>% 
  # ungroup() %>% 
  dplyr::select(region:n_Obs, sst, bottomT, sss, mld_cum, mld_1_cum, t2m, tcc_cum, p_e_cum, mslp_cum,
                lwr_budget, swr_budget, lhf_budget, shf_budget, qnet_budget)

# Join to the GLORYS MHW correlation results
events_cor_SOM <- left_join(events_cor_prep, SOM_info, by = c("region", "event_no"))

# Plotting function
plot_func <- function(df, name) {
  ggplot(data = df, aes(x = node, y = ts)) +
    geom_tile(aes(fill = value)) +
    # facet_wrap(~name, scales = "free") +
    scale_fill_gradient2(low = "blue", high = "red", name = name) +
    coord_cartesian(expand = F)
}

# Summary stats per node shown as heatmap
nested_SOM <- events_cor_SOM %>% 
  dplyr::select(-event_no) %>% 
  mutate(node = as.factor(node)) %>% 
  group_by(node, ts) %>% 
  summarise_if(is.numeric, mean) %>% 
  pivot_longer(cols = duration:count) %>%
  filter(name != "temp",
         ts != "full") %>%
  group_by(name) %>% 
  nest() %>% 
  mutate(plots = map2(data, name, plot_func)) 
# gridExtra::grid.arrange(grobs = nested_SOM$plots) # NB: The legends are too large for HTML

# Summary heatmap for correlation values only
events_cor_SOM %>% 
  dplyr::select(node, ts, bottomT:qnet_budget, -mld_1_cum) %>% 
  group_by(node, ts) %>% 
  summarise_if(is.numeric, mean) %>% 
  pivot_longer(cols = bottomT:qnet_budget) %>%
  filter(name != "temp",
         ts != "full") %>%
  ungroup() %>% 
  mutate(node = as.factor(node),
         ts = factor(ts, levels = c("decline", "full", "onset")),
         name = case_when(name == "sst" ~ "SST",
                         name == "bottomT" ~ "Bottom",
                         name == "sss" ~ "SSS",
                         name == "mld_cum" ~ "MLD",
                         name == "mld_1_cum" ~ "MLD_1_c",
                         name == "t2m" ~ "Air",
                         name == "tcc_cum" ~ "Cloud",
                         name == "p_e_cum" ~ "P_E",
                         name == "mslp_cum" ~ "MSLP",
                         name == "lwr_budget" ~ "Qlw",
                         name == "swr_budget" ~ "Qsw",
                         name == "lhf_budget" ~ "Qlh",
                         name == "shf_budget" ~ "Qsh",
                         name == "qnet_budget" ~ "Qnet",
                         TRUE ~ name),
         name = factor(name, levels = c("Qnet", "Qlw", "Qsw", "Qlh", "Qsh", "Cloud", 
                                        "P_E", "Air", "MSLP", "MLD", "SSS", "Bottom"))) %>% 
  ggplot(aes(x = name, y = ts)) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~node, scales = "free") +
  scale_fill_gradient2(low = "blue", high = "red") +
  coord_cartesian(expand = F) +
  labs(y = NULL, fill = "r (mean)") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 30))
```

Some important patterns come through when we look at the summary correlation and MHW metric results when grouped into their SOM nodes. This is as far as the numeric results will go. From here out it is necessary for a human to look at these summary results with the SOM node results to discern the meaning of the combined results.

And there we have our SOM results. Up next in the [Node summary vignette](https://robwschlegel.github.io/MHWflux/node-summary.html) we will show the results with a range of visuals.

## References

